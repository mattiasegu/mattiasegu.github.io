{% include base_path %}


<!-- BNE -->
<tr>
  <td rowspan="5" class="center" style="width: 400px;"><img src="images/thumbnails/bne.png" alt="bne" style="width:400px; height:auto;"></td>
  <td class="bold">Batch Normalization Embeddings for Deep Domain Generalization</td>
  </tr>
  <tr>
  <td class="italic"><span style="text-decoration: underline;">Mattia Segu</span>, Alessio Tonioni, Federico Tombari</td>
  </tr>
  <tr>
  <td class="thin">Pattern Recognition (PR), 2023</td>
  </tr>
  <tr>
  <td style="text-align: justify">Domain generalization aims at training machine learning models to perform robustly across different and unseen domains. Several recent methods use multiple datasets to train models to extract domain-invariant features, hoping to generalize to unseen domains. Instead, first we explicitly train domain-dependant representations by using ad-hoc batch normalization layers to collect independent domain's statistics. Then, we propose to use these statistics to map domains in a shared latent space, where membership to a domain can be measured by means of a distance function. At test time, we project samples from an unknown domain into the same space and infer properties of their domain as a linear combination of the known ones. We apply the same mapping strategy at training and test time, learning both a latent representation and a powerful but lightweight ensemble model. We show a significant increase in classification accuracy over current state-of-the-art techniques on popular domain generalization benchmarks: PACS, Office-31 and Office-Caltech.</td>
  </tr>
  <tr>
  <td style="padding-bottom: 5px;">
    <a href="https://arxiv.org/abs/2011.12672">[arXiv]</a>
    <a href="https://www.sciencedirect.com/science/article/pii/S0031320322005957?casa_token=h73VmmvzHEEAAAAA:rW0OW03XnNz4RYtI6JcrdnLIsLxP3BpDhw5rICen8KcyivqoGZ0PPJis-Uz-11ILrF4Ap9HWdw">[PR 2023 Paper]</a>
  </td>
  </tr>