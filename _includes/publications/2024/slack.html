{% include base_path %}

<!-- SLAck -->
<tr>
<td rowspan="5" class="center" style="width: 600px;"><img src="images/thumbnails/slack.png" alt="slack" style="width:600px; height:auto;"></td>
<td class="bold">SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking</td>
</tr>
<tr>
<td class="italic">Siyuan Li, Lei Ke, Yung-Hsu Yang, Luigi Piccinelli, <span style="text-decoration: underline;">Mattia Seg√π</span>, Martin Danelljan, Luc Van Gool</td>
</tr>
<tr>
<td class="thin">European Conference on Computer Vision (ECCV), 2024</td>
</tr>
<tr>
<td style="text-align: justify">Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to novel categories not in the training set. Currently, the best-performing methods are mainly based on pure appearance matching. Due to the complexity of motion patterns in the large-vocabulary scenarios and unstable classification of the novel objects, the motion and semantics cues are either ignored or applied based on heuristics in the final matching steps by existing methods. In this paper, we present a unified framework SLAck that jointly considers semantics location, and appearance priors in the early steps of association and learns how to integrate all valuable information through a lightweight spatial and temporal object graph. Our method eliminates complex post-processing heuristics for fusing different cues and boosts the association performance significantly for large-scale open-vocabulary tracking. Without bells and whistles, we outperform previous state-of-the-art methods for novel classes tracking on the open-vocabulary MOT and TAO TETA benchmarks.</td>
</tr>
<tr>
<td>
  <a href="https://arxiv.org/abs/2409.11235">[arXiv]</a>
  <a href="https://link.springer.com/chapter/10.1007/978-3-031-73383-3_1">[ECCV 2024 Paper]</a>
  <a href="https://github.com/siyuanliii/SLAck">[Code]</a>
</td>
</tr>

<!-- Empty row for spacing -->
<tr>
  <td style="height: 15px; padding: 0;" colspan="2"></td>
</tr>
